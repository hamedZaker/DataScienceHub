{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# clean data\ndf.drop(['Cabin'], axis=1, inplace=True) # dropping 'Cabin' column because it has a lot of null values.\ndf.fillna(df.median(), inplace=True) # fill in NA values with median\n\n# create validation set (stratified sampling)\ndf[\"Age_cat\"] = pd.cut(df[\"Age\"], bins=[0., 16, 32, 48, 64, np.inf], labels=[1, 2, 3, 4, 5])\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, valid_index in split.split(df, df[\"Age_cat\"]):\n    train = df.loc[train_index] \n    valid = df.loc[valid_index]\n    \ntrain.dropna(inplace=True) # delete rows with empty values (should only affect categorical column Embarked)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-03T16:41:35.447470Z","iopub.execute_input":"2021-09-03T16:41:35.447905Z","iopub.status.idle":"2021-09-03T16:41:36.530713Z","shell.execute_reply.started":"2021-09-03T16:41:35.447811Z","shell.execute_reply":"2021-09-03T16:41:36.529554Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"y = train['Survived'] # labels \nX = train.drop(['Survived', 'PassengerId', 'Name', 'Ticket'], 1, inplace=True) # drop the irrelevant columns and keep the rest\nX = pd.get_dummies(train, drop_first=True) # convert non-numerical variables to dummy variables\n# ----- #\nyp = valid['Survived'] # labels \nXp = valid.drop(['Survived', 'PassengerId', 'Name', 'Ticket'], 1, inplace=True) # drop the irrelevant columns and keep the rest\nXp = pd.get_dummies(valid, drop_first=True) # convert non-numerical variables to dummy variables","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-03T16:41:59.465296Z","iopub.execute_input":"2021-09-03T16:41:59.465691Z","iopub.status.idle":"2021-09-03T16:41:59.496528Z","shell.execute_reply.started":"2021-09-03T16:41:59.465659Z","shell.execute_reply":"2021-09-03T16:41:59.495350Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Decision trees\n# from sklearn import tree\n# dtc = tree.DecisionTreeClassifier()\n# dtc.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:28:45.358148Z","iopub.execute_input":"2021-09-01T18:28:45.358708Z","iopub.status.idle":"2021-09-01T18:28:45.362605Z","shell.execute_reply.started":"2021-09-01T18:28:45.358643Z","shell.execute_reply":"2021-09-01T18:28:45.361433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=6, random_state=2)\nclf.fit(X, y)\nprint(\"training accuracy = %.3f\" %clf.score(X, y), \"\\nvalidation accuracy = %.3f\" %clf.score(Xp, yp))\n# Re-build the forest with whole data \nclf.fit(pd.concat([X, Xp]), pd.concat([y, yp])) \nprint(\"accuracy = %.3f\" %clf.score(pd.concat([X, Xp]), pd.concat([y, yp])))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T16:42:01.606932Z","iopub.execute_input":"2021-09-03T16:42:01.607355Z","iopub.status.idle":"2021-09-03T16:42:02.126293Z","shell.execute_reply.started":"2021-09-03T16:42:01.607320Z","shell.execute_reply":"2021-09-03T16:42:02.125033Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"training accuracy = 0.859 \nvalidation accuracy = 0.872\naccuracy = 0.865\n","output_type":"stream"}]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nclf = XGBClassifier(\n learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 5,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\nclf.fit(X, y)\n# make predictions for test data\ny_pred = clf.predict(Xp)\nprint(\"training accuracy = %.3f\" %clf.score(X, y), \"\\nvalidation accuracy = %.3f\" %clf.score(Xp, yp))\n# Re-build the forest with whole data \nclf.fit(pd.concat([X, Xp]), pd.concat([y, yp])) \nprint(\"accuracy = %.3f\" %clf.score(pd.concat([X, Xp]), pd.concat([y, yp])))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T16:42:07.375888Z","iopub.execute_input":"2021-09-03T16:42:07.376399Z","iopub.status.idle":"2021-09-03T16:42:11.108280Z","shell.execute_reply.started":"2021-09-03T16:42:07.376365Z","shell.execute_reply":"2021-09-03T16:42:11.107327Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[16:42:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\ntraining accuracy = 0.928 \nvalidation accuracy = 0.860\n[16:42:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\naccuracy = 0.919\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nclf = Pipeline([ (\"scaler\", StandardScaler()), (\"svc\", svm.SVC(C=1000)),])\n# clf = svm.SVC(C=1000)\nclf.fit(X, y)\nprint(\"training accuracy = %.3f\" %clf.score(X, y), \"\\nvalidation accuracy = %.3f\" %clf.score(Xp, yp))\n# Re-build the forest with whole data\nclf.fit(pd.concat([X, Xp]), pd.concat([y, yp]))\nprint(\"accuracy = %.3f\" %clf.score(pd.concat([X, Xp]), pd.concat([y, yp])))","metadata":{"execution":{"iopub.status.busy":"2021-09-03T16:42:15.254517Z","iopub.execute_input":"2021-09-03T16:42:15.255236Z","iopub.status.idle":"2021-09-03T16:42:15.503712Z","shell.execute_reply.started":"2021-09-03T16:42:15.255196Z","shell.execute_reply":"2021-09-03T16:42:15.502622Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"training accuracy = 0.894 \nvalidation accuracy = 0.860\naccuracy = 0.897\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean and prepare test set\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\") # load the testing data\nids = test[['PassengerId']] # create a sub-dataset for submission file and saving it\ntest.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], 1, inplace=True) # drop the irrelevant columns\ntest.fillna(test.median(), inplace=True)\ntest[\"Age_cat\"] = pd.cut(test[\"Age\"], bins=[0., 16, 32, 48, 64, np.inf], labels=[1, 2, 3, 4, 5])\ntest = pd.get_dummies(test, drop_first=True) # convert non-numerical variables to dummy variables\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T16:42:26.463901Z","iopub.execute_input":"2021-09-03T16:42:26.464535Z","iopub.status.idle":"2021-09-03T16:42:26.496438Z","shell.execute_reply.started":"2021-09-03T16:42:26.464463Z","shell.execute_reply":"2021-09-03T16:42:26.495440Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"predictions = clf.predict(test)\nresults = ids.assign(Survived = predictions) # assign predictions to ids\nresults.to_csv(\"titanic-results.csv\", index=False) # write the final dataset to a csv file.\n","metadata":{"execution":{"iopub.status.busy":"2021-09-03T16:42:47.784692Z","iopub.execute_input":"2021-09-03T16:42:47.785105Z","iopub.status.idle":"2021-09-03T16:42:47.805211Z","shell.execute_reply.started":"2021-09-03T16:42:47.785069Z","shell.execute_reply":"2021-09-03T16:42:47.804246Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-09-01T18:04:46.052569Z","iopub.execute_input":"2021-09-01T18:04:46.053189Z","iopub.status.idle":"2021-09-01T18:04:46.060898Z","shell.execute_reply.started":"2021-09-01T18:04:46.053132Z","shell.execute_reply":"2021-09-01T18:04:46.06006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}