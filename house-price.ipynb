{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"House Prices - Advanced Regression Techniques\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T17:12:44.603075Z","iopub.execute_input":"2021-09-18T17:12:44.603853Z","iopub.status.idle":"2021-09-18T17:12:44.620237Z","shell.execute_reply.started":"2021-09-18T17:12:44.603694Z","shell.execute_reply":"2021-09-18T17:12:44.619011Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import category_encoders as ce\n\n# list of variables whose levels are not present partially in test (so to be removed, just for a rough solution)\nnotInTestList = ['RoofMatl','HouseStyle','Exterior2nd','BsmtFullBath','BsmtHalfBath', 'MiscFeature','Condition2','Heating','Electrical','GarageCars',\n                 'Exterior1st','PoolQC','TotRmsAbvGrd']\n\n# change to categorical/oridnal (from numerical)\nlistOfCategorical = ['MSSubClass','OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                     'TotRmsAbvGrd','Fireplaces','GarageCars']\nlistOfCategorical = [x for x in listOfCategorical if x not in notInTestList]\n\nlistOfOrdinal = ['LotShape','LandSlope','OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n                 'HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence']\nlistOfOrdinal = [x for x in listOfOrdinal if x not in notInTestList]\n\n\ndef outliers_iqr(ys):\n    quartile_1, quartile_3 = np.percentile(ys, [25, 75]) #Get 1st and 3rd quartiles (25% -> 75% of data will be kept)\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr * 1.5) #Get lower bound\n    upper_bound = quartile_3 + (iqr * 1.5) #Get upper bound\n    return np.where((ys > upper_bound) | (ys < lower_bound)) #Get outlier values\n    \ndef cleanDf(df): \n\n    for col in (listOfCategorical+listOfOrdinal): \n        frequencies = df[col].value_counts(normalize=True)\n        mapping = df[col].map(frequencies)\n        df[col].mask(mapping < (0.3/len(frequencies)), 'Other', inplace=True)\n    \n    for col in listOfCategorical:\n        df[col] = df[col].astype('category')\n    \n    # correlation analysis for numerical variables\n#     corrMat = df.corr()\n#     listOfFeatures = [i for i in corrMat]\n    # which numerical data is weakly correlated with Price?\n#     setOfLeastCorrelated = set() \n#     for f in listOfFeatures :\n#         if abs(corrMat[f]['SalePrice']) < 0.1: \n#                 setOfLeastCorrelated.add(f)\n    # setOfLeastCorrelated = {'3SsnPorch','BsmtFinSF2','Id', 'LowQualFinSF', 'MiscVal', 'MoSold', 'PoolArea', 'YrSold'}   \n    setOfLeastCorrelated = {'3SsnPorch','BsmtFinSF2','LowQualFinSF', 'MiscVal', 'PoolArea'}   \n    # But for now we keep YrSold and MoSold to use later for computing the Age of the home\n    df.drop(setOfLeastCorrelated, axis=1, inplace=True)\n\n    # which numerical data is strongly correlated with Price?\n#     setOfMostCorrelated = set() \n#     for f in listOfFeatures :\n#         if abs(corrMat[f]['SalePrice']) > 0.6: \n#                 setOfMostCorrelated.add(f)\n    # setOfMostCorrelated  = {'1stFlrSF', 'GarageArea', 'GrLivArea', 'SalePrice', 'TotalBsmtSF'}\n    \n#     listOfFeatures = [i for i in corrMat]\n#     setOfDroppedFeatures = set() \n#     for i in range(len(listOfFeatures)) :\n#         for j in range(i+1,len(listOfFeatures)): #Avoid repetitions \n#             feature1=listOfFeatures[i]\n#             feature2=listOfFeatures[j]\n#             if abs(corrMat[feature1][feature2]) > 0.8: #If the correlation between the features is > 0.8\n#                 setOfDroppedFeatures.add(feature1) #Add one of them to the set\n    # Someone tried different values of threshold and 0.8 was the one that gave the best results\n\n    # data = data.drop(setOfDroppedFeatures, axis=1)\n    # setOfDroppedFeatures = {'TotalBsmtSF', 'YearBuilt'} I do not touch YearBuilt for now\n    # TotalBsmtSF is highly correlated with 1stFlrSF and both of them are in the list of MostCorrelated\n    # For now, I don't delete any of them\n\n    # computing Age and remocing unnecessary columns\n    df['Age'] = df['YrSold'] - df['YearBuilt']\n    df['AgeOfRemodel'] = df['YrSold'] - df['YearRemodAdd']\n    df['YrSold'] = df['YrSold'].astype('category') # as there are only 5 yeras\n    df.drop(['YearRemodAdd','YearBuilt','MoSold'], axis=1, inplace=True)    \n    \n    # total surface\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    df.drop(['TotalBsmtSF','1stFlrSF','2ndFlrSF'], axis=1, inplace=True)    \n    \n    df.drop(notInTestList, axis=1, inplace=True)    \n    \n    # drop columns using t-test and wilcoxon test\n    setOfWeakestTtest = ['Street','Utilities']\n    df.drop(setOfWeakestTtest, axis=1, inplace=True)\n\n    # NA is meaningfull somwehre\n    df['Alley'].fillna(\"No alley access\") \n    df['BsmtQual'].fillna(\"No basement\") \n#     df = pd.get_dummies(df, columns=['BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'])\n    ordinalEncoder = ce.PolynomialEncoder(cols=listOfOrdinal, drop_invariant=True)\n    df = ordinalEncoder.fit_transform(df)\n    \n    df = pd.get_dummies(df, dummy_na=True)\n    # house co located are similar in size \n#     df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    df.fillna(0, inplace=True)\n           \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-09-18T17:13:58.561801Z","iopub.execute_input":"2021-09-18T17:13:58.562202Z","iopub.status.idle":"2021-09-18T17:13:59.770378Z","shell.execute_reply.started":"2021-09-18T17:13:58.562170Z","shell.execute_reply":"2021-09-18T17:13:59.769306Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\ntrain.drop('Id', axis=1, inplace=True)\ntrain = cleanDf(train)\nnewdf = train\nfor column in train:\n    outlierValuesList = np.ndarray.tolist(outliers_iqr(newdf[column])[0]) #outliers_iqr() returns an array\n    train = newdf.drop(outlierValuesList)\n\nids = test[['Id']]\ntest.drop('Id', axis=1, inplace=True)\ntest = cleanDf(test)\n\ndiff1List = list(set(train.keys()) - set(test.keys()))\ndiff1List.remove('SalePrice')\ndiff2List = list(set(test.keys()) - set(train.keys()))\ntest[diff1List] = 0\ntest.drop(diff2List, axis=1, inplace=True)\n\nnumericCols = list(train.select_dtypes(include = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).keys())\nnumericCols.remove('SalePrice')\n\n# random splitting\nfrom sklearn.model_selection import train_test_split\ntrain, dev = train_test_split(train, test_size=0.2, random_state=42)\n\nscalerX =StandardScaler()\nscalerX.fit(train[numericCols])\ntrain[numericCols] = scalerX.transform(train[numericCols])\ndev[numericCols] = scalerX.transform(dev[numericCols])\ntest[numericCols] = scalerX.transform(test[numericCols])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T17:28:40.850176Z","iopub.execute_input":"2021-09-18T17:28:40.850584Z","iopub.status.idle":"2021-09-18T17:28:43.899867Z","shell.execute_reply.started":"2021-09-18T17:28:40.850542Z","shell.execute_reply":"2021-09-18T17:28:43.898665Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  elif pd.api.types.is_categorical(cols):\n/opt/conda/lib/python3.7/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  elif pd.api.types.is_categorical(cols):\n","output_type":"stream"}]},{"cell_type":"code","source":"# scalery = StandardScaler()\n# scalery.fit(train[['SalePrice']])\ntrain['SalePrice'] = np.log1p(train[['SalePrice']])\ndev['SalePrice'] = np.log1p(dev[['SalePrice']])\nX = train.drop('SalePrice', axis=1)\ny = train['SalePrice']\nXp = dev.drop('SalePrice', axis=1)\nyp = dev['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2021-09-18T17:28:48.576135Z","iopub.execute_input":"2021-09-18T17:28:48.576484Z","iopub.status.idle":"2021-09-18T17:28:48.592338Z","shell.execute_reply.started":"2021-09-18T17:28:48.576456Z","shell.execute_reply":"2021-09-18T17:28:48.591164Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# load packages\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingRegressor\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:01:13.989775Z","iopub.execute_input":"2021-09-18T18:01:13.990247Z","iopub.status.idle":"2021-09-18T18:01:13.996447Z","shell.execute_reply.started":"2021-09-18T18:01:13.990210Z","shell.execute_reply":"2021-09-18T18:01:13.995092Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# linear regression\nregLin = LinearRegression()\nregLin.fit(X, y)\ny_pred = regLin.predict(X)\nyp_pred = regLin.predict(Xp)\nprint(\"training accuracy = %.3f\" %mean_squared_error(y.T, y_pred), \"\\nvalidation accuracy = %.3f\" %mean_squared_error(yp.T, yp_pred))\n# something is wrong but idk!","metadata":{"execution":{"iopub.status.busy":"2021-09-18T17:28:51.722716Z","iopub.execute_input":"2021-09-18T17:28:51.723096Z","iopub.status.idle":"2021-09-18T17:28:51.782216Z","shell.execute_reply.started":"2021-09-18T17:28:51.723062Z","shell.execute_reply":"2021-09-18T17:28:51.781366Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"training accuracy = 0.014 \nvalidation accuracy = 6786807782557477888.000\n","output_type":"stream"}]},{"cell_type":"code","source":"# support vector machine\nparam_grid = [ {'C': [1,10, 100, 1000]}, {'kernel': ['linear','poly','rbf']} ] \nregSVM = SVR()\nregSVM_grdSearch = GridSearchCV(regSVM, param_grid)\nregSVM_grdSearch.fit(X, y)\ny_pred = regSVM_grdSearch.predict(X)\nyp_pred = regSVM_grdSearch.predict(Xp)\nprint(\"training accuracy = %.3f\" %mean_squared_error(y.T, y_pred), \"\\nvalidation accuracy = %.3f\" %mean_squared_error(yp.T, yp_pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T17:40:01.541231Z","iopub.execute_input":"2021-09-18T17:40:01.541590Z","iopub.status.idle":"2021-09-18T17:40:42.944790Z","shell.execute_reply.started":"2021-09-18T17:40:01.541551Z","shell.execute_reply":"2021-09-18T17:40:42.943158Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"training accuracy = 0.018 \nvalidation accuracy = 0.019\n","output_type":"stream"}]},{"cell_type":"code","source":"# gradient boosting\nparam_grid = [ {'learning_rate': [0.1,0.05,0.01]}, {'n_estimators': [100,200,400]}, {'max_depth': range(3,10)} ] \nregBoost = GradientBoostingRegressor()\nregBoost_grdSearch = GridSearchCV(regBoost, param_grid)\nregBoost_grdSearch.fit(X, y)\ny_pred = regBoost_grdSearch.predict(X)\nyp_pred = regBoost_grdSearch.predict(Xp)\nprint(\"training accuracy = %.3f\" %mean_squared_error(y.T, y_pred), \"\\nvalidation accuracy = %.3f\" %mean_squared_error(yp.T, yp_pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T17:51:56.194830Z","iopub.execute_input":"2021-09-18T17:51:56.195247Z","iopub.status.idle":"2021-09-18T17:53:24.265679Z","shell.execute_reply.started":"2021-09-18T17:51:56.195212Z","shell.execute_reply":"2021-09-18T17:53:24.264487Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"training accuracy = 0.004 \nvalidation accuracy = 0.019\n","output_type":"stream"}]},{"cell_type":"code","source":"# random forest\nparam_grid = [ {'n_estimators': [100,200,400]}, {'max_depth': range(3,10)} ] \nregFoerst = RandomForestRegressor()\nregForest_grdSearch = GridSearchCV(regForest, param_grid)\nregForest_grdSearch.fit(X, y)\ny_pred = regForest_grdSearch.predict(X)\nyp_pred = regForest_grdSearch.predict(Xp)\nprint(\"training accuracy = %.3f\" %mean_squared_error(y.T, y_pred), \"\\nvalidation accuracy = %.3f\" %mean_squared_error(yp.T, yp_pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T17:55:54.725600Z","iopub.execute_input":"2021-09-18T17:55:54.725984Z","iopub.status.idle":"2021-09-18T17:58:40.963843Z","shell.execute_reply.started":"2021-09-18T17:55:54.725949Z","shell.execute_reply":"2021-09-18T17:58:40.962459Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"training accuracy = 0.003 \nvalidation accuracy = 0.023\n","output_type":"stream"}]},{"cell_type":"code","source":"# voting\nregVoting = VotingRegressor([('svm',regSVM_grdSearch), ('boost',regBoost_grdSearch), ('rf',regForest_grdSearch)])\nregVoting.fit(X, y)\ny_pred = regVoting.predict(X)\nyp_pred = regVoting.predict(Xp)\nprint(\"training accuracy = %.3f\" %mean_squared_error(y.T, y_pred), \"\\nvalidation accuracy = %.3f\" %mean_squared_error(yp.T, yp_pred))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:15:38.839978Z","iopub.execute_input":"2021-09-18T18:15:38.840527Z","iopub.status.idle":"2021-09-18T18:20:34.432464Z","shell.execute_reply.started":"2021-09-18T18:15:38.840478Z","shell.execute_reply":"2021-09-18T18:20:34.431426Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"training accuracy = 0.005 \nvalidation accuracy = 0.018\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = np.expm1(regSVM_grdSearch.predict(test))\nresults = ids.assign(SalePrice = predictions) # assign predictions to ids\nresults.to_csv(\"results.csv\", index=False) # write the final dataset to a csv file.","metadata":{"execution":{"iopub.status.busy":"2021-09-18T18:23:21.084991Z","iopub.execute_input":"2021-09-18T18:23:21.085505Z","iopub.status.idle":"2021-09-18T18:23:21.264027Z","shell.execute_reply.started":"2021-09-18T18:23:21.085474Z","shell.execute_reply":"2021-09-18T18:23:21.263219Z"},"trusted":true},"execution_count":84,"outputs":[]}]}